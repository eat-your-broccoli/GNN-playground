{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1143a9bc",
   "metadata": {},
   "source": [
    "# GCN on MUTAG dataset\n",
    "\n",
    "code taken from: \n",
    "- https://colab.research.google.com/drive/1mMUKnvM_Byu8wEcJpFSYGnniPPhIOD7N?usp=sharing#scrollTo=f4sXdEnBOVke\n",
    "- https://theaisummer.com/graph-convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e44eafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipynb in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from torch) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install ipynb\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51562fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchnet in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (0.0.4)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: visdom in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from torchnet) (0.1.8.9)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from torchnet) (1.16.0)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from torchnet) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from torch->torchnet) (4.1.1)\n",
      "Requirement already satisfied: torchfile in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (1.3.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (2.27.1)\n",
      "Requirement already satisfied: jsonpatch in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (1.32)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (1.7.3)\n",
      "Requirement already satisfied: tornado in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (6.1)\n",
      "Requirement already satisfied: pyzmq in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (22.3.0)\n",
      "Requirement already satisfied: numpy>=1.8 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (1.21.5)\n",
      "Requirement already satisfied: pillow in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from visdom->torchnet) (9.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from jsonpatch->visdom->torchnet) (2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from requests->visdom->torchnet) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from requests->visdom->torchnet) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from requests->visdom->torchnet) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\envs\\deeplearning\\lib\\site-packages (from requests->visdom->torchnet) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchnet networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2efcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.basics_laplace as basics_laplace\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25e27b",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3a90df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnet as tnt\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09d12e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are ready\n"
     ]
    }
   ],
   "source": [
    "def indices_to_one_hot(number, nb_classes, label_dummy=-1):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    if number == label_dummy:\n",
    "        return np.zeros(nb_classes)\n",
    "    else:\n",
    "        return np.eye(nb_classes)[number]\n",
    "\n",
    "def get_graph_signal(nx_graph):\n",
    "    d = dict((k, v) for k, v in nx_graph.nodes.items())\n",
    "    x = []\n",
    "    invd = {}\n",
    "    j = 0\n",
    "    for k, v in d.items():\n",
    "        x.append(v['attr_dict'])\n",
    "        invd[k] = j\n",
    "        j = j + 1\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "def load_data(path, ds_name, use_node_labels=True, max_node_label=10):\n",
    "    node2graph = {} # dictionary which node belongs to which graph\n",
    "    Gs = [] # list of all the graphs\n",
    "    data = []\n",
    "    dataset_graph_indicator = f\"{ds_name}_graph_indicator.txt\"\n",
    "    dataset_adj = f\"{ds_name}_A.txt\"\n",
    "    dataset_node_labels = f\"{ds_name}_node_labels.txt\"\n",
    "    dataset_graph_labels = f\"{ds_name}_graph_labels.txt\"\n",
    "\n",
    "    path_graph_indicator = os.path.join(path, dataset_graph_indicator)\n",
    "    path_adj = os.path.join(path, dataset_adj)\n",
    "    path_node_lab = os.path.join(path, dataset_node_labels)\n",
    "    path_labels = os.path.join(path, dataset_graph_labels)\n",
    "\n",
    "\n",
    "    #\n",
    "    # (2) DS_graph_indicator.txt (n lines)\n",
    "    # column vector of graph identifiers for all nodes of all graphs,\n",
    "    # the value in the i-th line is the graph_id of the node with node_id i\n",
    "    #\n",
    "    # TL;DR: we have a list of (node_id, graph_id), we want to build graph objects and assign the node-objects to them\n",
    "    #\n",
    "    \n",
    "    with open(path_graph_indicator, \"r\") as f:\n",
    "        c = 1\n",
    "        for line in f: # c is the node id, line is the id of the graph it belongs to\n",
    "            node2graph[c] = int(line[:-1])\n",
    "            if not node2graph[c] == len(Gs): # create a new graph if the line specifies a unseen graph\n",
    "                Gs.append(nx.Graph()) # add it to list of graphs\n",
    "            Gs[-1].add_node(c) # add the node to the graph\n",
    "            c += 1\n",
    "\n",
    "    #(1) DS_A.txt (m lines) \n",
    "    # sparse (block diagonal) adjacency matrix for all graphs,\n",
    "    # each line corresponds to (row, col) resp. (node_id, node_id)\n",
    "    #\n",
    "    # TL;DR: each line is (node_id, node_id), specifies an edge between two nodes \n",
    "    with open(path_adj, \"r\") as f:\n",
    "        for line in f:\n",
    "            edge = line[:-1].split(\",\")\n",
    "            edge[1] = edge[1].replace(\" \", \"\")\n",
    "            Gs[node2graph[int(edge[0])] - 1].add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "            \n",
    "    # (4) DS_node_labels.txt (n lines)\n",
    "    # column vector of node labels,\n",
    "    # the value in the i-th line corresponds to the node with node_id i\n",
    "    # \n",
    "    # TL;DR: the line-index is the node_id, the line itself is the index of the node type. convert this index to one-hot\n",
    "    if use_node_labels:\n",
    "        with open(path_node_lab, \"r\") as f:\n",
    "            c = 1\n",
    "            for line in f:\n",
    "                node_label = indices_to_one_hot(int(line[:-1]), max_node_label)\n",
    "                Gs[node2graph[c] - 1].add_node(c, attr_dict=node_label)\n",
    "                c += 1\n",
    "\n",
    "                \n",
    "    #(3) DS_graph_labels.txt (N lines) \n",
    "    # class labels for all graphs in the dataset,\n",
    "    #the value in the i-th line is the class label of the graph with graph_id i\n",
    "    #\n",
    "    # TL;DR the i-th line contains the label for the i-th graph\n",
    "    labels = []\n",
    "    with open(path_labels, \"r\") as f:\n",
    "        for line in f:\n",
    "            labels.append(int(line[:-1]))\n",
    "\n",
    "    # two lists, one containing the graphs, one containing the graph-labels\n",
    "    return list(zip(Gs, labels)) \n",
    "\n",
    "def create_loaders(dataset, batch_size, split_id, offset=-1):\n",
    "    train_dataset = dataset[:split_id]\n",
    "    val_dataset = dataset[split_id:]\n",
    "    return to_pytorch_dataset(train_dataset, offset,batch_size), to_pytorch_dataset(val_dataset, offset,batch_size)\n",
    "\n",
    "def to_pytorch_dataset(dataset, label_offset=0, batch_size=1):\n",
    "    list_set = []\n",
    "    for graph, label in dataset:\n",
    "        F, G = get_graph_signal(graph), nx.to_numpy_matrix(graph)\n",
    "        numOfNodes = G.shape[0]\n",
    "        F_tensor = torch.from_numpy(F).float()\n",
    "        G_tensor = torch.from_numpy(G).float()\n",
    "\n",
    "        # fix labels to zero-indexing\n",
    "        if label == -1:\n",
    "            label = 0\n",
    "    \n",
    "        label += label_offset\n",
    "    \n",
    "        list_set.append(tuple((F_tensor, G_tensor, label)))\n",
    "\n",
    "    dataset_tnt = tnt.dataset.ListDataset(list_set)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset_tnt, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_data(path='./MUTAG/', ds_name='MUTAG',\n",
    "                  use_node_labels=True, max_node_label=7)\n",
    "train_dataset, val_dataset = create_loaders(dataset, batch_size=1, split_id=150, offset=0)\n",
    "print('Data are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b7970",
   "metadata": {},
   "source": [
    "## Implementing a 1-hop GCN layer in Pytorch\n",
    "\n",
    "Y is the output. Lnorm is the Laplacian of the Adjacency Matrix A. X is the graph signal. W are the learnable parameters. D is the Degree-Matrix of A. I is the Identity Matrix.\n",
    "\n",
    "$$Y=Lnorm​XW$$\n",
    "$$L^{norm}mod​=D^{−1/2}​(A+I)D^{-1/2}​$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e5efe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34744ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run something on specific hardware, e.g. cuda\n",
    "def device_as(x,y):\n",
    "    return x.to(y.device)\n",
    "\n",
    "def calc_degree_matrix_norm(a):\n",
    "    return torch.diag_embed(torch.pow(a.sum(dim=-1),-0.5))\n",
    "\n",
    "def create_graph_lapl_norm(a):\n",
    "    size = a.shape[-1]\n",
    "    a +=  device_as(torch.eye(size),a)\n",
    "    D_norm = calc_degree_matrix_norm(a)\n",
    "    L_norm = torch.bmm( torch.bmm(D_norm, a) , D_norm )\n",
    "    return L_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4ac99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCN_AISUMMER extends nn.Module\n",
    "class GCN_AISUMMER(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        # Applies a linear transformation to the incoming data: y=x*A^(T)+b*y\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias) \n",
    "        \n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        A: adjecency matrix\n",
    "        X: graph signal\n",
    "        \"\"\"\n",
    "        L = create_graph_lapl_norm(A)\n",
    "        x = self.linear(X)\n",
    "        return torch.bmm(L, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ce97b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNN extends torch.nn.Module\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                    in_features = 7, # cora dataset has 7 different topics\n",
    "                    hidden_dim = 64,\n",
    "                    classes = 2,\n",
    "                    dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(GCN_AISUMMER(in_features, hidden_dim))\n",
    "        self.layers.append(GCN_AISUMMER(hidden_dim, hidden_dim))\n",
    "        self.layers.append(GCN_AISUMMER(hidden_dim, hidden_dim))\n",
    "        self.fc = nn.Linear(hidden_dim, classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, A):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,A)\n",
    "            x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # aggregate node embeddings\n",
    "        x = x.mean(dim=1)\n",
    "        # final classification layer\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "849ab179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 010, Train Acc: 0.6600, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 020, Train Acc: 0.6733, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 030, Train Acc: 0.6667, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 040, Train Acc: 0.7000, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 050, Train Acc: 0.6933, Val Acc: 0.6579 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 060, Train Acc: 0.7067, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 070, Train Acc: 0.7267, Val Acc: 0.6579 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 080, Train Acc: 0.7200, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 090, Train Acc: 0.7267, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 100, Train Acc: 0.6867, Val Acc: 0.6842 || Best Val Score: 0.7105 (Epoch 003) \n",
      "Epoch: 110, Train Acc: 0.7200, Val Acc: 0.6842 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 120, Train Acc: 0.7133, Val Acc: 0.7105 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 130, Train Acc: 0.7533, Val Acc: 0.6842 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 140, Train Acc: 0.7267, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 150, Train Acc: 0.7067, Val Acc: 0.6316 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 160, Train Acc: 0.7200, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 170, Train Acc: 0.7333, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 180, Train Acc: 0.7333, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 190, Train Acc: 0.7400, Val Acc: 0.6842 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 200, Train Acc: 0.7400, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 210, Train Acc: 0.7133, Val Acc: 0.7105 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 220, Train Acc: 0.7400, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 230, Train Acc: 0.7333, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "Epoch: 240, Train Acc: 0.7267, Val Acc: 0.6579 || Best Val Score: 0.7368 (Epoch 106) \n",
      "done training model!\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'Training on {device}')\n",
    "model = GNN(in_features = 7,\n",
    "                hidden_dim = 128,\n",
    "                classes = 2).to(device)\n",
    "\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader: \n",
    "        optimizer.zero_grad()  \n",
    "        X, A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Compute the graph classification loss.\n",
    "        loss = criterion(out, labels) \n",
    "        # Calculate gradients.\n",
    "        loss.backward()  \n",
    "        # Updates the models parameters\n",
    "        optimizer.step() \n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        X, A, labels = data\n",
    "        X, A, labels = X.to(device), A.to(device), labels.to(device)  \n",
    "        # Forward pass.\n",
    "        out = model(X, A)  \n",
    "        # Take the index of the class with the highest probability.\n",
    "        pred = out.argmax(dim=1) \n",
    "        # Compare with ground-truth labels.\n",
    "        correct += int((pred == labels).sum()) \n",
    "    return correct / len(loader.dataset)  \n",
    "\n",
    "best_val = -1\n",
    "for epoch in range(1, 241):\n",
    "    train(train_dataset)\n",
    "    train_acc = test(train_dataset)\n",
    "    val_acc = test(val_dataset)\n",
    "    if val_acc>best_val:\n",
    "        best_val = val_acc\n",
    "        epoch_best = epoch\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f} || Best Val Score: {best_val:.4f} (Epoch {epoch_best:03d}) ')\n",
    "        \n",
    "print('done training model!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
